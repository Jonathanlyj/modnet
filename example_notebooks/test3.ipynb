{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16f504ff-7d62-498d-b04f-37c8eed1a446",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/yll6162/miniconda/envs/modnet/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-09-07 21:40:02.875112: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-07 21:40:04.173649: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-07 21:40:04.173747: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-07 21:40:04.173759: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from jarvis.db.figshare import data\n",
    "from jarvis.core.atoms import Atoms\n",
    "import pandas as pd\n",
    "from modnet.featurizers.presets import DeBreuck2020Featurizer\n",
    "import os\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "import numpy as np\n",
    "from modnet.preprocessing import MODData\n",
    "from modnet.models import MODNetModel\n",
    "from pymatgen.core import Composition\n",
    "import warnings\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import tensorflow as tf\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6afd146-3a4e-412b-9ed9-daa1c24a9901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/yll6162/miniconda/envs/modnet/lib/python3.9/site-packages/modnet/featurizers/presets/debreuck_2020.py:41: UserWarning: The BasicFeaturizer preset was written for and tested only with matminer==0.6.2.\n",
      "Newer versions of matminer will not work, and older versions may not be compatible with newer MODNet versions due to other conflicts.\n",
      "To use this featurizer robustly, please install `modnet==0.1.13` with its pinned dependencies.\n",
      "\n",
      "This preset will now be initialised without importing matminer featurizers to enable use with existing previously featurized data, but attempts to perform further featurization will result in an error.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class BasicFeaturizer(DeBreuck2020Featurizer):\n",
    "    from pymatgen.analysis.local_env import VoronoiNN\n",
    "\n",
    "    from matminer.featurizers.composition import (\n",
    "        AtomicOrbitals,\n",
    "        ElementFraction,\n",
    "        ElementProperty,\n",
    "        Stoichiometry,\n",
    "        TMetalFraction,\n",
    "        ValenceOrbital,\n",
    "    )\n",
    "\n",
    "    from matminer.featurizers.structure import (\n",
    "        BondFractions,\n",
    "        ChemicalOrdering,\n",
    "        CoulombMatrix,\n",
    "        DensityFeatures,\n",
    "        EwaldEnergy,\n",
    "        GlobalSymmetryFeatures,\n",
    "        MaximumPackingEfficiency,\n",
    "        RadialDistributionFunction,\n",
    "        SineCoulombMatrix,\n",
    "        StructuralHeterogeneity,\n",
    "        XRDPowderPattern,\n",
    "    )\n",
    "\n",
    "    from matminer.featurizers.site import (\n",
    "        AGNIFingerprints,\n",
    "        AverageBondAngle,\n",
    "        AverageBondLength,\n",
    "        BondOrientationalParameter,\n",
    "        ChemEnvSiteFingerprint,\n",
    "        CoordinationNumber,\n",
    "        CrystalNNFingerprint,\n",
    "        GaussianSymmFunc,\n",
    "        GeneralizedRadialDistributionFunction,\n",
    "        LocalPropertyDifference,\n",
    "        OPSiteFingerprint,\n",
    "        VoronoiFingerprint,\n",
    "    )\n",
    "\n",
    "    oxid_composition_featurizers = ()\n",
    "\n",
    "    composition_featurizers = (\n",
    "        AtomicOrbitals(),\n",
    "        ElementFraction(),\n",
    "        ElementProperty.from_preset(\"magpie\"),\n",
    "        Stoichiometry(),\n",
    "        TMetalFraction(),\n",
    "        ValenceOrbital(),\n",
    "    )\n",
    "\n",
    "    site_featurizers = (\n",
    "        AGNIFingerprints(),\n",
    "        AverageBondAngle(VoronoiNN()),\n",
    "        AverageBondLength(VoronoiNN()),\n",
    "        BondOrientationalParameter(),\n",
    "        ChemEnvSiteFingerprint.from_preset(\"simple\"),\n",
    "        CoordinationNumber(),\n",
    "        CrystalNNFingerprint.from_preset(\"ops\"),\n",
    "        GaussianSymmFunc(),\n",
    "        GeneralizedRadialDistributionFunction.from_preset(\"gaussian\"),\n",
    "        LocalPropertyDifference(),\n",
    "        OPSiteFingerprint(),\n",
    "        VoronoiFingerprint(),\n",
    "    )\n",
    "\n",
    "basic_featurizer = BasicFeaturizer()\n",
    "basic_featurizer.set_n_jobs(20)\n",
    "# basic_featurizer._n_jobs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8188523-9b0b-4712-b233-ac043f313a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aflow_density', 'aflow_Egap', 'aflow_enthalpy_formation_atom', 'aflow_volume_atom', 'jarvis_bulk_modulus', 'jarvis_e_form', 'jarvis_gap_opt', 'jarvis_gap_tbmbj', 'jarvis_shear_modulus', 'mp_band_gap', 'mp_density', 'mp_e_above_hull', 'mp_formation_energy_per_atom', 'mp_total_magnetization', 'mp_volume', 'oqmd_band_gap', 'oqmd_e_formation_energy', 'oqmd_stability', 'oqmd_volume', '.ipynb_checkpoints']\n",
      "2023-09-07 00:15:31,958 - modnet - INFO - Multiprocessing on 1 workers.\n",
      "2023-09-07 00:15:31,961 - modnet - INFO - Computing \"self\" MI (i.e. information entropy) of features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 257/257 [00:00<00:00, 618.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-07 00:15:32,403 - modnet - INFO - Computing cross NMI between all features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████| 12561/12561 [00:18<00:00, 667.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-07 00:15:51,357 - modnet - INFO - Starting target 1/1: target ...\n",
      "2023-09-07 00:15:51,358 - modnet - INFO - Computing mutual information between features and target...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-07 00:15:51,967 - modnet - INFO - Computing optimal features...\n",
      "2023-09-07 00:15:54,525 - modnet - INFO - Selected 50/153 features...\n",
      "2023-09-07 00:15:56,154 - modnet - INFO - Selected 100/153 features...\n",
      "2023-09-07 00:15:56,817 - modnet - INFO - Selected 150/153 features...\n",
      "2023-09-07 00:15:56,826 - modnet - INFO - Done with target 1/1: target.\n",
      "2023-09-07 00:15:56,827 - modnet - INFO - Merging all features...\n",
      "2023-09-07 00:15:56,827 - modnet - INFO - Done.\n",
      "epoch 0: loss: 3710.969, val_loss:2159.019 val_mae:37.709\n",
      "epoch 1: loss: 3697.459, val_loss:2147.741 val_mae:37.565\n",
      "epoch 2: loss: 3683.234, val_loss:2136.312 val_mae:37.416\n",
      "epoch 3: loss: 3668.765, val_loss:2124.414 val_mae:37.259\n",
      "epoch 4: loss: 3653.584, val_loss:2111.759 val_mae:37.091\n",
      "epoch 5: loss: 3637.249, val_loss:2097.997 val_mae:36.906\n",
      "epoch 6: loss: 3619.516, val_loss:2082.836 val_mae:36.703\n",
      "epoch 7: loss: 3600.091, val_loss:2066.117 val_mae:36.478\n",
      "epoch 8: loss: 3578.537, val_loss:2047.610 val_mae:36.228\n",
      "epoch 9: loss: 3554.495, val_loss:2027.062 val_mae:35.949\n",
      "epoch 10: loss: 3527.744, val_loss:2004.293 val_mae:35.637\n",
      "epoch 11: loss: 3498.012, val_loss:1979.142 val_mae:35.290\n",
      "epoch 12: loss: 3465.073, val_loss:1951.450 val_mae:34.902\n",
      "epoch 13: loss: 3428.751, val_loss:1920.836 val_mae:34.476\n",
      "epoch 14: loss: 3388.560, val_loss:1887.239 val_mae:34.002\n",
      "epoch 15: loss: 3344.331, val_loss:1850.559 val_mae:33.492\n",
      "epoch 16: loss: 3295.814, val_loss:1810.565 val_mae:32.956\n",
      "epoch 17: loss: 3242.778, val_loss:1767.118 val_mae:32.382\n",
      "epoch 18: loss: 3184.908, val_loss:1720.146 val_mae:31.751\n",
      "epoch 19: loss: 3121.883, val_loss:1669.526 val_mae:31.063\n",
      "epoch 20: loss: 3053.514, val_loss:1615.113 val_mae:30.338\n",
      "epoch 21: loss: 2979.541, val_loss:1556.984 val_mae:29.577\n",
      "epoch 22: loss: 2899.912, val_loss:1495.269 val_mae:28.750\n",
      "epoch 23: loss: 2814.624, val_loss:1430.195 val_mae:27.857\n",
      "epoch 24: loss: 2723.679, val_loss:1362.161 val_mae:26.923\n",
      "epoch 25: loss: 2627.305, val_loss:1291.765 val_mae:25.978\n",
      "epoch 26: loss: 2525.963, val_loss:1219.988 val_mae:25.085\n",
      "epoch 27: loss: 2420.417, val_loss:1147.915 val_mae:24.206\n",
      "epoch 28: loss: 2311.750, val_loss:1076.975 val_mae:23.339\n",
      "epoch 29: loss: 2201.361, val_loss:1008.962 val_mae:22.499\n",
      "epoch 30: loss: 2090.849, val_loss:945.638 val_mae:21.945\n",
      "epoch 31: loss: 1981.969, val_loss:889.135 val_mae:21.524\n",
      "epoch 32: loss: 1876.833, val_loss:841.597 val_mae:21.187\n",
      "epoch 33: loss: 1778.192, val_loss:805.401 val_mae:21.019\n",
      "epoch 34: loss: 1688.921, val_loss:782.907 val_mae:21.145\n",
      "epoch 35: loss: 1611.754, val_loss:775.846 val_mae:21.463\n",
      "epoch 36: loss: 1549.122, val_loss:784.825 val_mae:22.030\n",
      "epoch 37: loss: 1502.702, val_loss:808.773 val_mae:22.796\n",
      "epoch 38: loss: 1472.761, val_loss:844.471 val_mae:23.540\n",
      "epoch 39: loss: 1457.568, val_loss:886.477 val_mae:24.225\n",
      "epoch 40: loss: 1453.242, val_loss:927.720 val_mae:24.931\n",
      "epoch 41: loss: 1454.130, val_loss:961.021 val_mae:25.592\n",
      "epoch 42: loss: 1454.137, val_loss:980.803 val_mae:25.943\n",
      "epoch 43: loss: 1448.208, val_loss:984.370 val_mae:25.984\n",
      "epoch 44: loss: 1433.522, val_loss:972.058 val_mae:25.738\n",
      "epoch 45: loss: 1409.806, val_loss:946.566 val_mae:25.304\n",
      "epoch 46: loss: 1378.884, val_loss:911.889 val_mae:24.737\n",
      "epoch 47: loss: 1343.763, val_loss:872.301 val_mae:24.108\n",
      "epoch 48: loss: 1307.608, val_loss:831.616 val_mae:23.446\n",
      "epoch 49: loss: 1273.183, val_loss:792.753 val_mae:22.795\n",
      "epoch 50: loss: 1242.395, val_loss:757.701 val_mae:22.155\n",
      "epoch 51: loss: 1216.216, val_loss:727.475 val_mae:21.541\n",
      "epoch 52: loss: 1194.739, val_loss:702.303 val_mae:20.993\n",
      "epoch 53: loss: 1177.406, val_loss:681.852 val_mae:20.547\n",
      "epoch 54: loss: 1163.346, val_loss:665.533 val_mae:20.180\n",
      "epoch 55: loss: 1151.416, val_loss:652.647 val_mae:19.866\n",
      "epoch 56: loss: 1140.541, val_loss:642.491 val_mae:19.604\n",
      "epoch 57: loss: 1129.788, val_loss:634.491 val_mae:19.383\n",
      "epoch 58: loss: 1118.465, val_loss:628.268 val_mae:19.200\n",
      "epoch 59: loss: 1106.149, val_loss:623.579 val_mae:19.051\n",
      "epoch 60: loss: 1092.700, val_loss:620.319 val_mae:18.933\n",
      "epoch 61: loss: 1078.198, val_loss:618.457 val_mae:18.841\n",
      "epoch 62: loss: 1062.791, val_loss:618.038 val_mae:18.773\n",
      "epoch 63: loss: 1046.849, val_loss:619.147 val_mae:18.730\n",
      "epoch 64: loss: 1030.736, val_loss:621.828 val_mae:18.700\n",
      "epoch 65: loss: 1014.877, val_loss:625.987 val_mae:18.691\n",
      "epoch 66: loss: 999.667, val_loss:631.460 val_mae:18.707\n",
      "epoch 67: loss: 985.345, val_loss:637.928 val_mae:18.742\n",
      "epoch 68: loss: 971.979, val_loss:644.989 val_mae:18.764\n",
      "epoch 69: loss: 959.523, val_loss:652.127 val_mae:18.834\n",
      "epoch 70: loss: 947.831, val_loss:658.817 val_mae:18.906\n",
      "epoch 71: loss: 936.663, val_loss:664.504 val_mae:18.956\n",
      "epoch 72: loss: 925.782, val_loss:668.674 val_mae:18.979\n",
      "epoch 73: loss: 914.948, val_loss:671.070 val_mae:18.957\n",
      "epoch 74: loss: 903.979, val_loss:671.655 val_mae:18.897\n",
      "epoch 75: loss: 892.783, val_loss:670.535 val_mae:18.802\n",
      "epoch 76: loss: 881.393, val_loss:667.969 val_mae:18.679\n",
      "epoch 77: loss: 869.898, val_loss:664.322 val_mae:18.528\n",
      "epoch 78: loss: 858.460, val_loss:660.023 val_mae:18.359\n",
      "epoch 79: loss: 847.223, val_loss:655.546 val_mae:18.185\n",
      "epoch 80: loss: 836.287, val_loss:651.290 val_mae:18.020\n",
      "epoch 81: loss: 825.714, val_loss:647.612 val_mae:17.867\n",
      "epoch 82: loss: 815.510, val_loss:644.703 val_mae:17.728\n",
      "epoch 83: loss: 805.643, val_loss:642.756 val_mae:17.623\n",
      "epoch 84: loss: 796.051, val_loss:641.910 val_mae:17.545\n",
      "epoch 85: loss: 786.612, val_loss:642.246 val_mae:17.494\n",
      "epoch 86: loss: 777.256, val_loss:643.794 val_mae:17.466\n",
      "epoch 87: loss: 767.896, val_loss:646.524 val_mae:17.464\n",
      "epoch 88: loss: 758.544, val_loss:650.339 val_mae:17.496\n",
      "epoch 89: loss: 749.202, val_loss:655.146 val_mae:17.549\n",
      "epoch 90: loss: 739.870, val_loss:660.817 val_mae:17.617\n",
      "epoch 91: loss: 730.600, val_loss:667.149 val_mae:17.696\n",
      "epoch 92: loss: 721.461, val_loss:673.876 val_mae:17.780\n",
      "epoch 93: loss: 712.508, val_loss:680.765 val_mae:17.865\n",
      "epoch 94: loss: 703.702, val_loss:687.577 val_mae:17.945\n",
      "epoch 95: loss: 695.070, val_loss:694.054 val_mae:18.019\n",
      "epoch 96: loss: 686.553, val_loss:699.985 val_mae:18.083\n",
      "epoch 97: loss: 678.161, val_loss:705.223 val_mae:18.136\n",
      "epoch 98: loss: 669.822, val_loss:709.712 val_mae:18.177\n",
      "epoch 99: loss: 661.510, val_loss:713.407 val_mae:18.203\n",
      "epoch 100: loss: 653.232, val_loss:716.370 val_mae:18.223\n",
      "epoch 101: loss: 645.025, val_loss:718.713 val_mae:18.234\n",
      "epoch 102: loss: 636.902, val_loss:720.640 val_mae:18.243\n",
      "epoch 103: loss: 628.859, val_loss:722.315 val_mae:18.250\n",
      "epoch 104: loss: 620.882, val_loss:723.922 val_mae:18.256\n",
      "epoch 105: loss: 613.009, val_loss:725.632 val_mae:18.265\n",
      "epoch 106: loss: 605.236, val_loss:727.593 val_mae:18.278\n",
      "epoch 107: loss: 597.541, val_loss:730.002 val_mae:18.297\n",
      "epoch 108: loss: 589.943, val_loss:732.913 val_mae:18.332\n",
      "epoch 109: loss: 582.403, val_loss:736.311 val_mae:18.376\n",
      "epoch 110: loss: 574.915, val_loss:740.079 val_mae:18.425\n",
      "epoch 111: loss: 567.458, val_loss:744.085 val_mae:18.475\n",
      "epoch 112: loss: 560.047, val_loss:748.194 val_mae:18.527\n",
      "epoch 113: loss: 552.693, val_loss:752.322 val_mae:18.577\n",
      "epoch 114: loss: 545.409, val_loss:756.347 val_mae:18.626\n",
      "epoch 115: loss: 538.180, val_loss:760.125 val_mae:18.674\n",
      "epoch 116: loss: 531.020, val_loss:763.656 val_mae:18.721\n",
      "epoch 117: loss: 523.941, val_loss:766.958 val_mae:18.763\n",
      "epoch 118: loss: 516.897, val_loss:769.862 val_mae:18.801\n",
      "epoch 119: loss: 509.871, val_loss:772.306 val_mae:18.832\n",
      "epoch 120: loss: 502.887, val_loss:774.288 val_mae:18.856\n",
      "epoch 121: loss: 495.969, val_loss:775.836 val_mae:18.878\n",
      "epoch 122: loss: 489.156, val_loss:777.053 val_mae:18.901\n",
      "epoch 123: loss: 482.470, val_loss:778.081 val_mae:18.924\n",
      "epoch 124: loss: 475.840, val_loss:778.871 val_mae:18.944\n",
      "epoch 125: loss: 469.270, val_loss:779.410 val_mae:18.959\n",
      "epoch 126: loss: 462.757, val_loss:779.743 val_mae:18.972\n",
      "epoch 127: loss: 456.274, val_loss:780.035 val_mae:18.985\n",
      "epoch 128: loss: 449.856, val_loss:780.525 val_mae:19.002\n",
      "epoch 129: loss: 443.506, val_loss:781.111 val_mae:19.020\n",
      "epoch 130: loss: 437.212, val_loss:781.632 val_mae:19.048\n",
      "epoch 131: loss: 430.985, val_loss:782.055 val_mae:19.076\n",
      "epoch 132: loss: 424.823, val_loss:782.315 val_mae:19.101\n",
      "epoch 133: loss: 418.739, val_loss:782.288 val_mae:19.121\n",
      "epoch 134: loss: 412.733, val_loss:781.972 val_mae:19.136\n",
      "epoch 135: loss: 406.781, val_loss:781.480 val_mae:19.147\n",
      "epoch 136: loss: 400.891, val_loss:780.843 val_mae:19.156\n",
      "epoch 137: loss: 395.024, val_loss:779.963 val_mae:19.159\n",
      "epoch 138: loss: 389.074, val_loss:778.864 val_mae:19.159\n",
      "epoch 139: loss: 383.207, val_loss:777.641 val_mae:19.157\n",
      "epoch 140: loss: 377.564, val_loss:776.581 val_mae:19.155\n",
      "epoch 141: loss: 372.010, val_loss:775.537 val_mae:19.152\n",
      "epoch 142: loss: 366.562, val_loss:774.819 val_mae:19.152\n",
      "epoch 143: loss: 361.236, val_loss:774.558 val_mae:19.156\n",
      "epoch 144: loss: 355.990, val_loss:774.463 val_mae:19.166\n",
      "epoch 145: loss: 350.826, val_loss:774.416 val_mae:19.181\n",
      "epoch 146: loss: 345.708, val_loss:774.185 val_mae:19.195\n",
      "epoch 147: loss: 340.692, val_loss:773.821 val_mae:19.206\n",
      "epoch 148: loss: 335.762, val_loss:773.170 val_mae:19.211\n",
      "epoch 149: loss: 330.900, val_loss:772.208 val_mae:19.210\n",
      "epoch 150: loss: 326.119, val_loss:771.068 val_mae:19.205\n",
      "epoch 151: loss: 321.417, val_loss:769.885 val_mae:19.196\n",
      "epoch 152: loss: 316.826, val_loss:768.845 val_mae:19.188\n",
      "epoch 153: loss: 312.305, val_loss:767.858 val_mae:19.186\n",
      "epoch 154: loss: 307.829, val_loss:767.003 val_mae:19.184\n",
      "epoch 155: loss: 303.406, val_loss:766.211 val_mae:19.182\n",
      "epoch 156: loss: 299.067, val_loss:765.349 val_mae:19.178\n",
      "epoch 157: loss: 294.786, val_loss:764.333 val_mae:19.171\n",
      "epoch 158: loss: 290.595, val_loss:763.085 val_mae:19.159\n",
      "epoch 159: loss: 286.460, val_loss:761.738 val_mae:19.145\n",
      "epoch 160: loss: 282.353, val_loss:760.206 val_mae:19.127\n",
      "epoch 161: loss: 278.352, val_loss:759.004 val_mae:19.112\n",
      "epoch 162: loss: 274.443, val_loss:757.886 val_mae:19.097\n",
      "epoch 163: loss: 270.610, val_loss:757.213 val_mae:19.096\n",
      "epoch 164: loss: 266.803, val_loss:756.702 val_mae:19.095\n",
      "epoch 165: loss: 263.019, val_loss:756.236 val_mae:19.093\n",
      "epoch 166: loss: 259.325, val_loss:755.883 val_mae:19.090\n",
      "epoch 167: loss: 255.727, val_loss:755.721 val_mae:19.090\n",
      "epoch 168: loss: 252.166, val_loss:755.690 val_mae:19.097\n",
      "epoch 169: loss: 248.694, val_loss:755.558 val_mae:19.103\n",
      "epoch 170: loss: 245.299, val_loss:755.366 val_mae:19.108\n",
      "epoch 171: loss: 241.934, val_loss:755.013 val_mae:19.108\n",
      "epoch 172: loss: 238.647, val_loss:754.660 val_mae:19.108\n",
      "epoch 173: loss: 235.400, val_loss:754.224 val_mae:19.110\n",
      "epoch 174: loss: 232.167, val_loss:754.083 val_mae:19.115\n",
      "epoch 175: loss: 228.970, val_loss:754.002 val_mae:19.124\n",
      "epoch 176: loss: 225.845, val_loss:753.708 val_mae:19.130\n",
      "epoch 177: loss: 222.725, val_loss:753.429 val_mae:19.135\n",
      "epoch 178: loss: 219.625, val_loss:753.053 val_mae:19.139\n",
      "epoch 179: loss: 216.644, val_loss:752.841 val_mae:19.145\n",
      "epoch 180: loss: 213.663, val_loss:752.700 val_mae:19.152\n",
      "epoch 181: loss: 210.718, val_loss:752.547 val_mae:19.157\n",
      "epoch 182: loss: 207.830, val_loss:752.195 val_mae:19.162\n",
      "epoch 183: loss: 205.007, val_loss:752.344 val_mae:19.173\n",
      "epoch 184: loss: 202.253, val_loss:753.073 val_mae:19.192\n",
      "epoch 185: loss: 199.490, val_loss:753.799 val_mae:19.210\n",
      "epoch 186: loss: 196.786, val_loss:754.476 val_mae:19.227\n",
      "epoch 187: loss: 194.074, val_loss:754.810 val_mae:19.236\n",
      "epoch 188: loss: 191.369, val_loss:755.085 val_mae:19.242\n",
      "epoch 189: loss: 188.709, val_loss:755.119 val_mae:19.242\n",
      "epoch 190: loss: 186.080, val_loss:755.096 val_mae:19.243\n",
      "epoch 191: loss: 183.401, val_loss:755.009 val_mae:19.246\n",
      "epoch 192: loss: 180.796, val_loss:755.505 val_mae:19.252\n",
      "epoch 193: loss: 178.165, val_loss:756.174 val_mae:19.258\n",
      "epoch 194: loss: 175.549, val_loss:756.693 val_mae:19.260\n",
      "epoch 195: loss: 172.952, val_loss:756.972 val_mae:19.259\n",
      "epoch 196: loss: 170.391, val_loss:756.730 val_mae:19.252\n",
      "epoch 197: loss: 167.823, val_loss:756.398 val_mae:19.244\n",
      "epoch 198: loss: 165.369, val_loss:756.021 val_mae:19.237\n",
      "epoch 199: loss: 162.942, val_loss:756.352 val_mae:19.239\n",
      "epoch 200: loss: 160.535, val_loss:757.191 val_mae:19.248\n",
      "epoch 201: loss: 158.219, val_loss:758.108 val_mae:19.259\n",
      "epoch 202: loss: 155.943, val_loss:759.015 val_mae:19.269\n",
      "epoch 203: loss: 153.657, val_loss:759.615 val_mae:19.274\n",
      "epoch 204: loss: 151.395, val_loss:760.312 val_mae:19.283\n",
      "epoch 205: loss: 149.150, val_loss:761.378 val_mae:19.296\n",
      "epoch 206: loss: 146.956, val_loss:762.660 val_mae:19.314\n",
      "epoch 207: loss: 144.764, val_loss:763.963 val_mae:19.331\n",
      "epoch 208: loss: 142.539, val_loss:765.128 val_mae:19.346\n",
      "epoch 209: loss: 140.394, val_loss:766.055 val_mae:19.356\n",
      "epoch 210: loss: 138.233, val_loss:766.945 val_mae:19.366\n",
      "epoch 211: loss: 136.128, val_loss:767.876 val_mae:19.376\n",
      "epoch 212: loss: 134.054, val_loss:768.823 val_mae:19.389\n",
      "epoch 213: loss: 131.950, val_loss:769.525 val_mae:19.403\n",
      "epoch 214: loss: 129.869, val_loss:770.053 val_mae:19.413\n",
      "epoch 215: loss: 127.850, val_loss:771.152 val_mae:19.428\n",
      "epoch 216: loss: 125.881, val_loss:772.783 val_mae:19.448\n",
      "epoch 217: loss: 123.904, val_loss:774.639 val_mae:19.470\n",
      "epoch 218: loss: 121.968, val_loss:776.253 val_mae:19.489\n",
      "epoch 219: loss: 120.015, val_loss:777.411 val_mae:19.504\n",
      "epoch 220: loss: 118.087, val_loss:778.004 val_mae:19.511\n",
      "epoch 221: loss: 116.195, val_loss:778.470 val_mae:19.515\n",
      "epoch 222: loss: 114.290, val_loss:779.042 val_mae:19.520\n",
      "epoch 223: loss: 112.400, val_loss:780.108 val_mae:19.534\n",
      "epoch 224: loss: 110.509, val_loss:781.076 val_mae:19.545\n",
      "epoch 225: loss: 108.609, val_loss:782.671 val_mae:19.564\n",
      "epoch 226: loss: 106.769, val_loss:784.632 val_mae:19.588\n",
      "epoch 227: loss: 104.948, val_loss:786.356 val_mae:19.609\n",
      "epoch 228: loss: 103.128, val_loss:787.581 val_mae:19.623\n",
      "epoch 229: loss: 101.321, val_loss:788.683 val_mae:19.635\n",
      "epoch 230: loss: 99.552, val_loss:789.358 val_mae:19.643\n",
      "epoch 231: loss: 97.744, val_loss:789.949 val_mae:19.650\n",
      "epoch 232: loss: 96.022, val_loss:791.236 val_mae:19.666\n",
      "epoch 233: loss: 94.343, val_loss:792.672 val_mae:19.682\n",
      "epoch 234: loss: 92.604, val_loss:794.193 val_mae:19.698\n",
      "epoch 235: loss: 90.869, val_loss:795.328 val_mae:19.706\n",
      "epoch 236: loss: 89.185, val_loss:795.594 val_mae:19.703\n",
      "epoch 237: loss: 87.555, val_loss:795.762 val_mae:19.699\n",
      "epoch 238: loss: 85.930, val_loss:796.416 val_mae:19.703\n",
      "epoch 239: loss: 84.324, val_loss:797.852 val_mae:19.718\n",
      "epoch 240: loss: 82.764, val_loss:799.547 val_mae:19.734\n",
      "epoch 241: loss: 81.228, val_loss:801.075 val_mae:19.749\n",
      "epoch 242: loss: 79.711, val_loss:802.340 val_mae:19.762\n",
      "epoch 243: loss: 78.278, val_loss:803.012 val_mae:19.771\n",
      "epoch 244: loss: 76.822, val_loss:803.782 val_mae:19.782\n",
      "epoch 245: loss: 75.352, val_loss:804.340 val_mae:19.792\n",
      "epoch 246: loss: 73.920, val_loss:806.038 val_mae:19.815\n",
      "epoch 247: loss: 72.477, val_loss:808.177 val_mae:19.842\n",
      "epoch 248: loss: 71.067, val_loss:810.043 val_mae:19.862\n",
      "epoch 249: loss: 69.711, val_loss:811.263 val_mae:19.874\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "----------------------------------------\n",
      "jarvis_shear_modulus: 18.417230012350082\n",
      "{'100': ['jarvis_bulk_modulus', 'jarvis_shear_modulus']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def iterate_dataset(folder_path):\n",
    "    dataset = []\n",
    "    for root, subfolders, files in os.walk(folder_path):\n",
    "        dataset.append(subfolders)\n",
    "    return dataset[0]\n",
    "\n",
    "\n",
    "file_path = \"/scratch/yll6162/modnet/ibrnet_data\"\n",
    "with open(os.path.join(file_path, \"feature_grp.json\"), 'r') as json_file:\n",
    "    feature_grp = json.load(json_file)\n",
    "\n",
    "datasets = iterate_dataset(file_path)\n",
    "# LOOP \n",
    "target_name = \"target\"\n",
    "mae_dic = {}\n",
    "print(datasets)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "tf.config.set_visible_devices(physical_devices[1:], 'GPU')\n",
    "\n",
    "# target_property = datasets[0]\n",
    "# target_property = \"jarvis_bulk_modulus\"\n",
    "target_property = \"jarvis_shear_modulus\"\n",
    "df_train = pd.read_csv(os.path.join(file_path, target_property, \"train.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(file_path, target_property, \"test.csv\"))\n",
    "df_val = pd.read_csv(os.path.join(file_path, target_property, \"val.csv\"))\n",
    "key = str(df_train.shape[0])\n",
    "featurize = key not in feature_grp.keys()\n",
    "\n",
    "\n",
    "df_train[\"composition\"] = df_train[\"formula\"].map(Composition) # maps composition to a pymatgen composition object\n",
    "df_test[\"composition\"] = df_test[\"formula\"].map(Composition)\n",
    "df_val[\"composition\"] = df_val[\"formula\"].map(Composition) # maps composition to a pymatgen composition object\n",
    "with tf.device('/device:GPU:1'):\n",
    "    if featurize:\n",
    "        feature_grp[key]= [target_property]\n",
    "        data_train = MODData(materials = df_train[\"composition\"],\n",
    "                       targets = df_train[target_name],\n",
    "                       target_names=[target_name],\n",
    "                       featurizer=basic_featurizer,\n",
    "                       structure_ids=df_train.formula)\n",
    "    \n",
    "        data_train.featurize()\n",
    "        data_train.feature_selection(n=200)\n",
    "        data_train.df_featurized.to_csv(os.path.join(file_path, f\"df_train_featurized_{key}.csv\"))\n",
    "    \n",
    "        \n",
    "        data_val = MODData(materials = df_val[\"composition\"],\n",
    "                       targets = df_val[target_name],\n",
    "                       target_names=[target_name],\n",
    "                       featurizer=basic_featurizer,\n",
    "                       structure_ids=df_val.formula)\n",
    "        data_val.featurize()\n",
    "        data_val.df_featurized.to_csv(os.path.join(file_path, f\"df_val_featurized_{key}.csv\"))\n",
    "        \n",
    "    else:\n",
    "        feature_grp[key].append(target_property)\n",
    "    \n",
    "        df_train_featurized = pd.read_csv(os.path.join(file_path, f\"df_train_featurized_{key}.csv\"), index_col = 0)\n",
    "        data_train = MODData(materials = df_train[\"composition\"],\n",
    "                             targets = df_train[target_name],\n",
    "                             target_names=[target_name],\n",
    "                             df_featurized = df_train_featurized,\n",
    "                             structure_ids=df_train_featurized.index)\n",
    "        data_train.feature_selection(n=200)\n",
    "        \n",
    "        df_val_featurized = pd.read_csv(os.path.join(file_path, f\"df_val_featurized_{key}.csv\"), index_col = 0)\n",
    "        data_val = MODData(materials = df_val[\"composition\"],\n",
    "                           targets = df_val[target_name],\n",
    "                           target_names=[target_name],\n",
    "                           df_featurized = df_val_featurized,\n",
    "                           structure_ids=df_val_featurized.index)\n",
    "    \n",
    "    model = MODNetModel([[[target_name]]],\n",
    "                        weights={target_name:1},\n",
    "                        num_neurons=[[256],[64],[64],[32]],\n",
    "                       )\n",
    "    model.fit(data_train,\n",
    "              val_data = data_val,\n",
    "              epochs = 250,\n",
    "              verbose = 1\n",
    "             )\n",
    "    if featurize:\n",
    "        data_to_predict = MODData(materials = df_test[\"composition\"],\n",
    "                       featurizer=basic_featurizer,\n",
    "                       structure_ids=df_test.formula)\n",
    "        data_to_predict.featurize()\n",
    "        data_to_predict.df_featurized.to_csv(os.path.join(file_path, f\"df_pred_featurized_{key}.csv\"))\n",
    "\n",
    "    else:\n",
    "        df_pred_featurized = pd.read_csv(os.path.join(file_path, f\"df_pred_featurized_{key}.csv\"), index_col = 0)\n",
    "        data_to_predict = MODData(materials = df_test[\"composition\"],\n",
    "                                  df_featurized = df_pred_featurized,\n",
    "                                  structure_ids=df_pred_featurized.index)\n",
    "    \n",
    "\n",
    "df_predictions = model.predict(data_to_predict)\n",
    "df_test_pred = df_test.merge(df_predictions, how = 'left', left_on = \"formula\", right_index = True, suffixes=('_true', '_pred'))\n",
    "mae = mean_absolute_error(df_test_pred[target_name+'_true'].values,df_test_pred[target_name+'_pred'].values)\n",
    "print(\"-\" * 40)\n",
    "print(f\"{target_property}: {mae}\")\n",
    "if os.path.exists(os.path.join(file_path, \"mae_all.csv\")):\n",
    "    df_mae_all = pd.read_csv(os.path.join(file_path, \"mae_all.csv\"), index_col = 0)\n",
    "    new_entry = {'target': target_property, 'mae': mae}\n",
    "    df_mae_all.loc[len(df_mae_all)] = new_entry\n",
    "else:\n",
    "    mae_dic = {'target': [target_property], 'mae': [mae]}\n",
    "    df_mae_all = pd.DataFrame.from_dict(mae_dic)\n",
    "df_mae_all.to_csv(os.path.join(file_path, \"mae_all.csv\"))\n",
    "feature_grp_str = json.dumps(feature_grp)\n",
    "with open(os.path.join(file_path, \"feature_grp.json\"), 'w') as json_file:\n",
    "    json_file.write(feature_grp_str)\n",
    "print(feature_grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff0677a0-3bbe-4c10-9c45-f529fccbcd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aflow_density', 'aflow_Egap', 'aflow_enthalpy_formation_atom', 'aflow_volume_atom', 'jarvis_bulk_modulus', 'jarvis_e_form', 'jarvis_gap_opt', 'jarvis_gap_tbmbj', 'jarvis_shear_modulus', 'mp_band_gap', 'mp_density', 'mp_e_above_hull', 'mp_formation_energy_per_atom', 'mp_total_magnetization', 'mp_volume', 'oqmd_band_gap', 'oqmd_e_formation_energy', 'oqmd_stability', 'oqmd_volume', '.ipynb_checkpoints']\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def iterate_dataset(folder_path):\n",
    "    dataset = []\n",
    "    for root, subfolders, files in os.walk(folder_path):\n",
    "        dataset.append(subfolders)\n",
    "    return dataset[0]\n",
    "\n",
    "\n",
    "file_path = \"/scratch/yll6162/modnet/ibrnet_data\"\n",
    "with open(os.path.join(file_path, \"feature_grp.json\"), 'r') as json_file:\n",
    "    feature_grp = json.load(json_file)\n",
    "\n",
    "datasets = iterate_dataset(file_path)\n",
    "# LOOP \n",
    "target_name = \"target\"\n",
    "mae_dic = {}\n",
    "print(datasets)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "tf.config.set_visible_devices(physical_devices[1:], 'GPU')\n",
    "\n",
    "# target_property = datasets[0]\n",
    "# target_property = \"jarvis_bulk_modulus\"\n",
    "target_property = \"jarvis_shear_modulus\"\n",
    "df_train = pd.read_csv(os.path.join(file_path, target_property, \"train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0ca67ce-b86f-4d72-a877-d43ab7f0e02a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>formula</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     formula  target\n",
       "1760     NaN    0.76"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_rows = df_train.isna().any(axis=1)\n",
    "df_train[nan_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "412a350a-b3d7-4699-82f7-127653225ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n",
      "----------------------------------------\n",
      "jarvis_bulk_modulus: 26.92448291236877\n",
      "{'100': ['jarvis_bulk_modulus'], 100: ['jarvis_bulk_modulus']}\n"
     ]
    }
   ],
   "source": [
    "df_predictions = model.predict(data_to_predict)\n",
    "df_test_pred = df_test.merge(df_predictions, how = 'left', left_on = \"formula\", right_index = True, suffixes=('_true', '_pred'))\n",
    "mae = mean_absolute_error(df_test_pred[target_name+'_true'].values,df_test_pred[target_name+'_pred'].values)\n",
    "print(\"-\" * 40)\n",
    "print(f\"{target_property}: {mae}\")\n",
    "# mae_dic[target_property] = mae\n",
    "# df_test_pred.to_csv(os.path.join(file_path, target_property, \"test_pred.csv\"))   \n",
    "# df_mae_all = pd.from_dict(mae_dic)\n",
    "# df_mae_all.to_csv(os.path.join(file_path, \"mae_all.csv\"))  \n",
    "feature_grp_str = json.dumps(feature_grp)\n",
    "with open(os.path.join(file_path, \"feature_grp.json\"), 'w') as json_file:\n",
    "    json_file.write(feature_grp_str)\n",
    "print(feature_grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c788e369-35a4-4637-abbe-d63078317c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>formula</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Si1W2Yb1</td>\n",
       "      <td>63.6023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Be2Li2Zn3</td>\n",
       "      <td>84.9053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Be1Mn1Si1V1</td>\n",
       "      <td>42.7211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pa1Sb1V2</td>\n",
       "      <td>71.8008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ho1Nd1Pd1</td>\n",
       "      <td>85.1632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279551</th>\n",
       "      <td>Na1Ni2Pa1</td>\n",
       "      <td>68.6927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279552</th>\n",
       "      <td>Ba1Sb2Tb1</td>\n",
       "      <td>129.4200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279553</th>\n",
       "      <td>Ac1Pb2Re1</td>\n",
       "      <td>99.8546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279554</th>\n",
       "      <td>Nb2Ni1Ru1</td>\n",
       "      <td>61.6680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279555</th>\n",
       "      <td>Co2Sb2Sc1V1</td>\n",
       "      <td>207.4550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>279556 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            formula    target\n",
       "0          Si1W2Yb1   63.6023\n",
       "1         Be2Li2Zn3   84.9053\n",
       "2       Be1Mn1Si1V1   42.7211\n",
       "3          Pa1Sb1V2   71.8008\n",
       "4         Ho1Nd1Pd1   85.1632\n",
       "...             ...       ...\n",
       "279551    Na1Ni2Pa1   68.6927\n",
       "279552    Ba1Sb2Tb1  129.4200\n",
       "279553    Ac1Pb2Re1   99.8546\n",
       "279554    Nb2Ni1Ru1   61.6680\n",
       "279555  Co2Sb2Sc1V1  207.4550\n",
       "\n",
       "[279556 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc2118f7-a4e5-4251-b94f-0608f1b58aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aflow_density</td>\n",
       "      <td>0.244915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aflow_Egap</td>\n",
       "      <td>0.122328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aflow_enthalpy_formation_atom</td>\n",
       "      <td>0.072389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aflow_volume_atom</td>\n",
       "      <td>0.844885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jarvis_gap_tbmbj</td>\n",
       "      <td>0.533603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>oqmd_band_gap</td>\n",
       "      <td>0.076796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>oqmd_e_formation_energy</td>\n",
       "      <td>0.097109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>oqmd_stability</td>\n",
       "      <td>0.087722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>oqmd_volume</td>\n",
       "      <td>25.060371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jarvis_bulk_modulus</td>\n",
       "      <td>13.108102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>jarvis_e_form</td>\n",
       "      <td>0.126679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>jarvis_gap_opt</td>\n",
       "      <td>0.329469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>jarvis_shear_modulus</td>\n",
       "      <td>12.131477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mp_band_gap</td>\n",
       "      <td>0.452048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mp_density</td>\n",
       "      <td>0.449443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mp_e_above_hull</td>\n",
       "      <td>0.139965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mp_formation_energy_per_atom</td>\n",
       "      <td>0.177399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mp_total_magnetization</td>\n",
       "      <td>1.705534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mp_volume</td>\n",
       "      <td>251.830085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           target         mae\n",
       "0                   aflow_density    0.244915\n",
       "1                      aflow_Egap    0.122328\n",
       "2   aflow_enthalpy_formation_atom    0.072389\n",
       "3               aflow_volume_atom    0.844885\n",
       "4                jarvis_gap_tbmbj    0.533603\n",
       "5                   oqmd_band_gap    0.076796\n",
       "6         oqmd_e_formation_energy    0.097109\n",
       "7                  oqmd_stability    0.087722\n",
       "8                     oqmd_volume   25.060371\n",
       "9             jarvis_bulk_modulus   13.108102\n",
       "10                  jarvis_e_form    0.126679\n",
       "11                 jarvis_gap_opt    0.329469\n",
       "12           jarvis_shear_modulus   12.131477\n",
       "13                    mp_band_gap    0.452048\n",
       "14                     mp_density    0.449443\n",
       "15                mp_e_above_hull    0.139965\n",
       "16   mp_formation_energy_per_atom    0.177399\n",
       "17         mp_total_magnetization    1.705534\n",
       "18                      mp_volume  251.830085"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mae_all = pd.read_csv(os.path.join(file_path, \"mae_all.csv\"), index_col = 0)\n",
    "lst = ['aflow_density', 'aflow_Egap', 'aflow_enthalpy_formation_atom', 'aflow_volume_atom', 'jarvis_bulk_modulus', 'jarvis_e_form', 'jarvis_gap_opt', 'jarvis_gap_tbmbj', 'jarvis_shear_modulus', 'mp_band_gap', 'mp_density', 'mp_e_above_hull', 'mp_formation_energy_per_atom', 'mp_total_magnetization', 'mp_volume', 'oqmd_band_gap', 'oqmd_e_formation_energy', 'oqmd_stability', 'oqmd_volume']\n",
    "df_mae_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f705d407-78ea-4043-a795-ef7440a446d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in lst:\n",
    "    if e not in df_mae_all.target.values:\n",
    "        print(\"'\" + e + \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0873a1-4167-4fe2-bf07-c69854a1fbc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
